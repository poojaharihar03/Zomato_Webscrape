{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c1fe7b",
   "metadata": {},
   "source": [
    "## Zomato web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70f03752",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'authority': 'scrapeme.live',\n",
    "    'dnt': '1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,/;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'sec-fetch-site': 'none',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "}  # this is use to tell zomato website from which source or which user is accessing their website because zomato use secure network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12629294",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (4233771392.py, line 117)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [5]\u001b[1;36m\u001b[0m\n\u001b[1;33m    data.to_csv('Hydrabad_menu.csv', index=False)\u001b[0m\n\u001b[1;37m                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "# Selenium setup\n",
    "driver = webdriver.Chrome(executable_path=r\"C:\\Users\\Pooja\\Desktop\\workspace\\rebel\\chromedriver.exe\")\n",
    "driver.get(\"https://www.zomato.com/bangalore\")\n",
    "time.sleep(2)\n",
    "\n",
    "scroll_pause_time = 3\n",
    "screen_height = driver.execute_script(\"return window.screen.height;\")\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))\n",
    "    i += 1\n",
    "    time.sleep(scroll_pause_time)\n",
    "\n",
    "    scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "    if (screen_height) * i > scroll_height:\n",
    "        break\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "divs1 = soup.findAll('div', class_='sc-1mo3ldo-0')\n",
    "divs = divs1[4:]\n",
    "\n",
    "urls = []\n",
    "rest_name = []\n",
    "ratings = []\n",
    "addresses = []\n",
    "price = []\n",
    "cuisine = []\n",
    "data = pd.DataFrame()\n",
    "\n",
    "for parent in divs:\n",
    "    try:\n",
    "        isCom = True\n",
    "        try:\n",
    "            if \"Inspiration for your first order\" in parent.find(\"h3\").text:\n",
    "                print(\"no need first\")\n",
    "                isCom = False\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if \"Best Food in\" in parent.find(\"h1\").text:\n",
    "                print(\"no need best\")\n",
    "                isCom = False\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if \"Top brands for\" in parent.find(\"h3\").text:\n",
    "                print(\"no need top\")\n",
    "                isCom = False\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if \"End of search results\" in parent.find(\"h3\").text:\n",
    "                print(\"no need end\")\n",
    "                isCom = False\n",
    "                break\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        if isCom:\n",
    "            divas = parent.findAll('div', class_='jumbo-tracker')\n",
    "            \n",
    "            \n",
    "\n",
    "            for par in divas:\n",
    "                link_tag = par.find(\"a\")\n",
    "                base = \"https://www.zomato.com\"\n",
    "\n",
    "                if 'href' in link_tag.attrs:\n",
    "                    link = link_tag.get('href')\n",
    "\n",
    "                url = urljoin(base, link)\n",
    "                urls.append(url)\n",
    "\n",
    "                rating_tag = par.div.a.next_sibling.div.div.div.div.div.div.div.text\n",
    "                price_tag = par.div.a.next_sibling.p.next_sibling.text\n",
    "                cuisine_tag = par.div.a.next_sibling.p.text\n",
    "\n",
    "                ratings.append(rating_tag)\n",
    "                price.append(price_tag)\n",
    "                cuisine.append(cuisine_tag)\n",
    "\n",
    "                driver.get(url)\n",
    "                time.sleep(3)\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "                rest_namea = [soup.select('h1.sc-7kepeu-0')[-1].text]\n",
    "                rest_name.extend(rest_namea)\n",
    "\n",
    "                cuisine_name = [h.text for h in soup.find_all('h4', class_=\"sc-1s0saks-15 iSmBPS\")]\n",
    "                priceL = [h1.text for h1 in soup.find_all('span', class_=\"sc-17hyc2s-1 cCiQWA\")]\n",
    "                description = [h2.text for h2 in soup.find_all('p', class_=\"sc-1s0saks-12 hcROsL\")]\n",
    "                \n",
    "                if len(rest_namea) == len(cuisine_name) == len(priceL) == len(description):\n",
    "                    data1 = pd.DataFrame(zip(rest_namea * len(cuisine_name), cuisine_name, priceL, description))\n",
    "                    data = data.append(data1, ignore_index=True)\n",
    "                else:\n",
    "                    print(\"Skipping data entry due to inconsistent lengths\")\n",
    "\n",
    "\n",
    "                if data.empty:\n",
    "\n",
    "                    data = pd.DataFrame(zip(rest_namea * len(cuisine_name), cuisine_name, priceL, description))\n",
    "                else:\n",
    "                    data1 = pd.DataFrame(zip(rest_namea * len(cuisine_name), cuisine_name, priceL, description))\n",
    "                    data = data.append(data1, ignore_index=True)\n",
    "\n",
    "                data.to_csv('fmenu.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6499245e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
